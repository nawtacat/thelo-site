// ─────────────────────────────────────────────────────────────
//   Google Cloud Functions – Thelo Practice (Full Application)
// ─────────────────────────────────────────────────────────────
const functions = require("firebase-functions");
const admin = require("firebase-admin");
const cors = require("cors")({ origin: true });

// ✅ Import the Google Cloud Translation client
const { TranslationServiceClient } = require('@google-cloud/translate').v3beta1;
const translationClient = new TranslationServiceClient();

// ✅ *** ADDED *** Import the Google Cloud Text-to-Speech client
// We initialize it once here to be used by all functions
const { TextToSpeechClient } = require("@google-cloud/text-to-speech");
const ttsClient = new TextToSpeechClient();

// ✅ Get your project ID from the environment
const projectId = process.env.GCLOUD_PROJECT;

// ✅ OpenAI + Firebase v2 Functions
const { OpenAI } = require("openai");
// ✅ *** MODIFIED *** Import 'onRequest' for 2nd Gen HTTP functions
const { onCall, onRequest } = require("firebase-functions/v2/https");
const { onSchedule } = require("firebase-functions/v2/scheduler");
const { onDocumentWritten } = require("firebase-functions/v2/firestore");
const { defineSecret } = require("firebase-functions/params");

if (!admin.apps.length) admin.initializeApp();

const db = admin.firestore();
// ✅ Standardized the secret variable name for consistency across all functions
const openAIKey = defineSecret("OPENAI_API_KEY");

exports.analyzeMathDrawing = onCall({ secrets: [openAIKey], region: 'us-central1', timeoutSeconds: 120 }, async (request) => {
    // 1. Authentication and validation
    if (!request.auth) {
        console.warn("Unauthenticated user analyzed a drawing — allowed in local-only mode.");
    }
    
    // ✅ Extract NEW inputs: userImage AND questionImage
    const { userImage, questionImage, problemText, correctAnswer, solutionSteps, customInstructions } = request.data || {};
    
    const requestedLanguageCode = request.data.language || 'en';

    // Check for userImage (Student Work) specifically
    if (!userImage || !problemText || !correctAnswer) {
        throw new functions.https.HttpsError(
            "invalid-argument",
            'Missing required fields: "userImage", "problemText", and "correctAnswer".'
        );
    }

    const openai = new OpenAI({ apiKey: openAIKey.value() });

    // 2. The Expert Tutor Prompt (Updated for Multi-Modal Context)
    const combinedPrompt = `
You are an expert, meticulous, and encouraging AI math tutor named Thelo.
You are provided with **two distinct visual inputs** (if a question image is present):
1. **REFERENCE IMAGE:** The question context (diagram, graph, or printed text).
2. **STUDENT WORK:** The student's handwriting (transparent overlay).

Your task is to determine if the Student Work correctly solves the problem presented in the Reference Image/Text.

**IMPORTANT: You must write your feedback in simple, clear ENGLISH.**
Your feedback will be programmatically translated.

**CONTEXT:**
* **Problem:** "${problemText}"
* **Correct Answer:** "${correctAnswer}"
* **Steps:** "${solutionSteps || 'N/A'}"
* **Instructions:** "${customInstructions || 'N/A'}"

**CRITICAL ANALYSIS RULES:**
1. **Visual Overlay:** Imagine the "Student Work" is placed on top of the "Reference Image".
2. **Numerical Match:** Compare the student's final number against "${correctAnswer}". Ignore units.
3. **Lucky Guess Check:** If the answer matches but the logic is flawed (e.g., perimeter vs area), mark INCORRECT.
4. **Output:** JSON ONLY: {"status": "correct" | "incorrect", "feedback": "..."}
`;

    // 3. Construct the Dynamic Payload
    // We build the content array step-by-step to handle the optional question image
    const contentPayload = [];

    // A. Add the System Prompt
    contentPayload.push({
        type: "input_text",
        text: combinedPrompt
    });

    // B. Add the Question Image (Context) - Only if it exists
    if (questionImage) {
        contentPayload.push({
            type: "input_text",
            text: "--- IMAGE 1: REFERENCE QUESTION (The Diagram) ---"
        });
        contentPayload.push({
            type: "input_image",
            image_url: questionImage,
            "detail": "high" // High detail is important for reading graphs/geometry
        });
    }

    // C. Add the Student Image (Solution) - Always present
    contentPayload.push({
        type: "input_text",
        text: "--- IMAGE 2: STUDENT HANDWRITING (The Attempt) ---"
    });
    contentPayload.push({
        type: "input_image",
        image_url: userImage,
        "detail": "low" // Low detail saves tokens and is usually sufficient for handwriting
    });


    try {
        // 4. Call the OpenAI API (Using your specific structure)
        const response = await openai.responses.create({
            model: "gpt-5-mini", 
            input: [
                {
                    role: "user",
                    content: contentPayload // Passing the dynamic array we built above
                },
            ],
            max_output_tokens: 1500,
        });

        // 5. Parse the model's feedback
        const rawResponse = response.output_text || '{}';
        let aiResult;

        try {
            const jsonMatch = rawResponse.match(/\{.*\}/s);
            const jsonString = jsonMatch ? jsonMatch[0] : rawResponse;
            aiResult = JSON.parse(jsonString);
        } catch (parseError) {
            console.error("Failed to parse JSON from AI response.", parseError);
            console.error("Raw AI Response:", rawResponse);
            throw new Error("AI response was not in the expected JSON format.");
        }
        
        if (!aiResult.status || !aiResult.feedback) {
            console.error("AI response missing required keys. Raw response:", rawResponse);
            throw new Error("AI response was missing 'status' or 'feedback' keys.");
        }
        
        // 6. Translate the feedback if needed (Your existing logic)
        let finalFeedback = aiResult.feedback; 
        const sourceLanguageCode = 'en';

        if (requestedLanguageCode.toLowerCase() !== sourceLanguageCode) {
            console.log(`Translating feedback from ${sourceLanguageCode} to ${requestedLanguageCode}...`);
            try {
                const [translateResponse] = await translationClient.translateText({
                    parent: `projects/${projectId}/locations/global`,
                    contents: [finalFeedback],
                    mimeType: 'text/plain',
                    sourceLanguageCode: sourceLanguageCode,
                    targetLanguageCode: requestedLanguageCode,
                });

                if (translateResponse.translations && translateResponse.translations.length > 0) {
                    finalFeedback = translateResponse.translations[0].translatedText;
                    console.log("Translation successful.");
                }
            } catch (translateError) {
                console.error("Error during translation:", translateError);
            }
        }

        // 7. Return the final result
        return {
            status: aiResult.status,
            feedback: finalFeedback
        };
    } catch (error) {
        console.error("Error in analyzeMathDrawing function:", error.message);
        throw new functions.https.HttpsError(
            "internal",
            "The AI analysis failed. Please try again."
        );
    }
});
// ─────────────────────────────────────────────────────────────
// ✅ analyzeMathDrawingForTutor — (Already 2nd Gen, no change)
// ─────────────────────────────────────────────────────────────
/**
 * Analyzes a student's handwritten math drawing for a tutor/parent...
 */
exports.analyzeMathDrawingForTutor = onCall({ secrets: [openAIKey], region: 'us-central1', timeoutSeconds: 120 }, async (request) => {
    // 1. Authentication and validation
    if (!request.auth) {
        console.warn("Unauthenticated user analyzed a drawing — allowed in local-only mode.");
    }
    const { imageBase64, problemText, correctAnswer, solutionSteps, customInstructions } = request.data || {};
    
    const requestedLanguageCode = request.data.language || 'en';

    if (!imageBase64 || !problemText || !correctAnswer) {
        throw new functions.https.HttpsError(
            "invalid-argument",
            'Missing required fields: "imageBase64", "problemText", and "correctAnswer".'
        );
    }

    const openai = new OpenAI({ apiKey: openAIKey.value() });

    // 2. The Expert Tutor Prompt (Third-Person Version)
  // This prompt instructs the AI to report on the student's work for a tutor.
    const combinedPrompt = `
You are an expert, meticulous AI math tutor assistant named Thelo.
Your primary task is to analyze a student's handwritten work and report your findings to a teacher or parent.

**IMPORTANT: You must write your feedback in simple, clear ENGLISH, and in the THIRD PERSON.**
For example, instead of "You wrote...", say "The student wrote...". Instead of "Your answer is correct", say "The student's answer is correct".
Your feedback will be programmatically translated to other languages, so it is critical that you use simple sentences.

**CONTEXT FOR YOU (THE ASSISTANT):**
* **The Problem Was:** "${problemText}"
* **The Correct Final Answer Is:** "${correctAnswer}"
* **Official Solution Steps (for your reference only):** "${solutionSteps || 'Not provided.'}"
* **Custom Instructions / Common Pitfalls:** "${customInstructions || 'Not provided.'}"

**⭐═══ THE MOST CRITICAL RULE: The "Lucky Guess" Check ═══⭐**
Your highest priority is to detect when a student arrives at the correct final answer using flawed logic. You must meticulously scrutinize the steps. For example, if the problem is finding the area of a 4x4 square (16) and the student writes "4+4+4+4=16", this is incorrect because they calculated the perimeter. You MUST mark this as "incorrect".

**⭐═══ NEW CRITICAL RULE: Numerical Answer Matching (Ignore Units) ═══⭐**
Before evaluating the logic, you must first read the final answer written by the student. **Focus *only* on the numerical value.**
* **IGNORE UNITS:** You must ignore any units of measure (like 'm', 'cm', '$', 'մ', etc.) written next to the number.
* **COMPARE NUMBERS:** Compare the *number* you read from the image to the correct numerical answer provided ("${correctAnswer}").
* **Example:** If the correct answer is "300", and the student writes "300m", "300մ", or just "300", you MUST consider the number a match.
* **Mismatch:** If the number you read from the image is NOT an exact match (e.g., student writes "600" but the answer is "6000"), you MUST immediately classify the status as "incorrect".

**YOUR TASK: Analyze the student's work in the image and respond with a JSON object. Your entire response must be ONLY the JSON object, with no preamble or explanations. The JSON object must have this exact format: {"status": "correct" | "incorrect", "feedback": "..."}**

**═══ ANALYSIS RULES ═══**
1.  **Image shows steps, answer is CORRECT:** If the logic is sound, status is "correct". If logic is flawed (the "lucky guess" scenario), status is "incorrect" and you must explain the logical error.
2.  **Image shows steps, answer is INCORRECT:** Status is "incorrect". Find the first mistake and explain it.
3.  **Image shows ONLY the final answer:** If correct, status is "correct" but note that they should show their work. If incorrect, status is "incorrect".
`;

    try {
        // 3. Call the OpenAI API
        const response = await openai.responses.create({
            model: "gpt-5-mini",
            input: [ { role: "user", content: [ { type: "input_text", text: combinedPrompt, }, { type: "input_image", image_url: imageBase64, "detail": "low", }, ], }, ],
            max_output_tokens: 1500,
        });

        // 4. Parse the model's feedback
        const rawResponse = response.output_text || '{}';
        let aiResult;
        try {
            const jsonMatch = rawResponse.match(/\{.*\}/s);
            const jsonString = jsonMatch ? jsonMatch[0] : rawResponse;
            aiResult = JSON.parse(jsonString);
        } catch (parseError) {
            console.error("Failed to parse JSON from AI response.", parseError);
            console.error("Raw AI Response:", rawResponse);
            throw new Error("AI response was not in the expected JSON format.");
        }
        
        if (!aiResult.status || !aiResult.feedback) {
            console.error("AI response missing required keys. Raw response:", rawResponse);
            throw new Error("AI response was missing 'status' or 'feedback' keys.");
        }
        
        // 5. Translate the feedback if needed
        let finalFeedback = aiResult.feedback;
        if (requestedLanguageCode.toLowerCase() !== 'en') {
            console.log(`Translating tutor feedback from en to ${requestedLanguageCode}...`);
            try {
                const [translateResponse] = await translationClient.translateText({
                    parent: `projects/${projectId}/locations/global`,
                    contents: [finalFeedback],
                    mimeType: 'text/plain',
                    sourceLanguageCode: 'en',
                    targetLanguageCode: requestedLanguageCode,
                });

              
                if (translateResponse.translations && translateResponse.translations.length > 0) {
                    finalFeedback = translateResponse.translations[0].translatedText;
                    console.log("Tutor feedback translation successful.");
                }
            } catch (translateError) {
                console.error("Error during tutor feedback translation:", translateError);
                // Fallback: If translation fails, just send the English feedback.
                // This is better than failing the whole function.
            }
        }

        // 6. Return the final result
        return { status: aiResult.status, feedback: finalFeedback };
    } catch (error) {
        console.error("Error in analyzeMathDrawingForTutor function:", error.message);
        throw new functions.https.HttpsError("internal", "The AI analysis failed. Please try again.");
    }
});
// ─────────────────────────────────────────────────────────────
// ✅ *** MODIFIED *** speakText – Migrated to 2nd Gen onRequest
// ─────────────────────────────────────────────────────────────
exports.speakText = onRequest({ region: 'us-central1', cors: true }, async (req, res) => {
    // We no longer need the `cors(req, res, ...)` wrapper
    // The `cors: true` in the config handles it.
    
    const { text, languageCode, voiceName, speakingRate } = req.body;
    if (!text || typeof text !== "string") {
      return res.status(400).json({ error: "Missing or invalid 'text' field" });
    }
    
    // We no longer need to `require` the client here
    // We use the global `ttsClient` defined at the top
    try {
      const [response] = await ttsClient.synthesizeSpeech({
        input: { text },
        voice: { languageCode: languageCode || "en-US", name: voiceName || "en-US-Wavenet-D" },
        audioConfig: { audioEncoding: "MP3", speakingRate: speakingRate || 1.0 }
      });
      res.set("Content-Type", "audio/mpeg").send(response.audioContent);
    } catch (error) {
      console.error("TTS error:", error);
      res.status(500).json({ error: "TTS failed." });
    }
});

// ─────────────────────────────────────────────────────────────
// ✅ *** MODIFIED *** initializeUserProfile – Migrated to 2nd Gen onCall
// ─────────────────────────────────────────────────────────────
exports.initializeUserProfile = onCall({ region: 'us-central1' }, async (request) => {
    // 1st Gen `(data)` becomes 2nd Gen `(request)`
    // The data is now inside `request.data`
    const data = request.data;
    if (!data.uid || !data.email || !data.selectedPlan) {
        throw new functions.https.HttpsError("invalid-argument", "Missing required fields.");
    }
    
    const { uid, email, name, selectedPlan } = data;
    const userRef = db.collection("users").doc(uid);
    const doc = await userRef.get();
    
    if (doc.exists) {
        await userRef.update({
        plan: selectedPlan,
        lastPlanSelectionAt: admin.firestore.FieldValue.serverTimestamp()
        });
        return { success: true, message: "User profile updated.", userId: uid };
    }
    
    await userRef.set({
        email,
        name: name || null,
        plan: selectedPlan,
        subscriptionStatus: "active",
        createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    return { success: true, message: "User profile created.", userId: uid };
});



// ─────────────────────────────────────────────────────────────
// ✅ explainBookText — (Already 2nd Gen, no change)
// ─────────────────────────────────────────────────────────────
exports.explainBookText = onCall({ secrets: [openAIKey], region: 'us-central1' }, async (request) => {
    // 1. Authentication and Validation
    if (!request.auth) {
        throw new functions.https.HttpsError("unauthenticated", "You must be signed in.");
    }

    const { text, bookTitle, contextText } = request.data;
    if (!text || !contextText) {
        throw new functions.https.HttpsError("invalid-argument", 'Request must include "text" and "contextText".');
    }

    // 2. Advanced Prompt Engineering for Non-Native Speakers
    const openai = new OpenAI({ apiKey: openAIKey.value() });
    
    const systemPrompt = `
You are an expert language tutor named Thelo, specializing in helping non-native English speakers.
Your student is an intermediate English learner (CEFR B1 level).
Your goal is to explain a selected text *within its original context* using simple, clear language.

**RULES:**
1.  **Be Brief:** Your main explanation must be ONE or TWO short sentences.
2.  **Use Simple English:** Avoid jargon, idioms, or complex sentence structures.
3.  **Focus:** Explain only the *selected text*. Use the context to understand it, but do not explain the whole context.
4.  **JSON Output:** Respond with a JSON object with this exact structure:
    {"simpleExplanation": "...", "contextualMeaning": "...", "keywords": [{"term": "...", "definition": "..."}]}

**JSON FIELD INSTRUCTIONS:**
-   "simpleExplanation": The main, simple definition of the selected text.
-   "contextualMeaning": A single sentence explaining what the selected text means *in this specific situation*.
-   "keywords": An array identifying 1-2 important words from the *selected text* with very simple definitions. If the selection is just one word, this array should contain only that word.
`;

    const userMessage = `
The student is reading: "${bookTitle || 'a book'}".

The full context from the page is:
"${contextText}"

The specific text the student selected to be explained is:
"${text}"

Please provide the explanation in the required JSON format.
`;

    try {
        const response = await openai.chat.completions.create({
            model: "gpt-4o", // This model is fast, cheap, and smart enough for this task
            messages: [
                { role: "system", content: systemPrompt },
                { role: "user", content: userMessage },
            ],
            temperature: 0.1, // Low temperature for more predictable, factual explanations
            max_tokens: 400,
            response_format: { type: "json_object" },
        });

        const content = response.choices[0]?.message?.content;
        if (!content) {
            throw new Error("AI response was empty.");
        }
        
        const aiResult = JSON.parse(content);

        // 3. Validate and return the structured JSON
        if (!aiResult.simpleExplanation) {
            throw new Error("AI response did not contain the required 'simpleExplanation' field.");
        }

        return { explanation: aiResult };

    } catch (error) {
        console.error("Error in explainBookText:", error);
        throw new functions.https.HttpsError("internal", "Failed to get an explanation. Please try again.");
    }
});

// ─────────────────────────────────────────────────────────────
// ⭐⭐⭐ FINAL KAFFETISH FUNCTION (Simple, OpenAI-Voiced) ⭐⭐⭐
// ─────────────────────────────────────────────────────────────
/**
 * Generates a *concise* AI explanation for a text selection using A2-B1 English
 * and returns it as high-quality, spoken audio (MP3 base64) using OpenAI's API.
 *
 * This function handles the *initial* "Speak" button press.
 */
exports.getSpokenExplanation = onCall({ secrets: [openAIKey], region: 'us-central1' }, async (request) => {
    // 1. Authentication and Validation
    if (!request.auth) {
        throw new functions.https.HttpsError("unauthenticated", "You must be signed in.");
    }

    const { text, bookTitle, contextText } = request.data;
    if (!text || !contextText) {
        throw new functions.https.HttpsError("invalid-argument", 'Request must include "text" and "contextText".');
    }

    // 2. === Step 1: Get the simple (A2-B1) text explanation ===
    const openai = new OpenAI({ apiKey: openAIKey.value() });
    
    const systemPrompt = `
You are an expert lexicographer (dictionary expert). A student has selected text from a book and wants a brief spoken explanation.
Your *entire response* must be *only* the text to be spoken.

**YOUR RULES:**
1.  **If the selected text is a single word:** Provide *only* its definition. Use the context to determine the *correct* meaning (e.g., 'bank' of a river vs. 'bank' for money), but do not mention the context itself.
2.  **If the selected text is a phrase or sentence:** Briefly explain its meaning in simple, direct terms.
3.  **CRITICAL: NO TEMPLATES.** Do NOT use any introductions or conversational phrases like "This means...".
4.  **NEW CRITICAL RULE: USE A2-B1 ENGLISH:** The student is an A2-B1 level (Beginner/Intermediate) English learner. You **must** use simple vocabulary and short sentences.

**Example 1 (Single Word):**
User selects: "lethargic"
Your response: "Having very little energy; feeling tired or lazy."

**Example 2 (Phrase):**
User selects: "he was at his wit's end"
Your response: "He was very worried and did not know what to do next."
`;

    const userMessage = `
The student is reading: "${bookTitle || 'a book'}".
The full context from the page is: "${contextText}"
The specific text the student selected to be explained is: "${text}"
Provide only the text to be spoken, following the rules.
`;

    let explanationText = ""; // This will hold the text we want to speak

    try {
        const response = await openai.chat.completions.create({
            model: "gpt-4o-mini",
            messages: [
                { role: "system", content: systemPrompt },
                { role: "user", content: userMessage },
            ],
            temperature: 0.1,
            max_tokens: 150,
        });

        explanationText = response.choices[0]?.message?.content?.trim();
        
        if (!explanationText) {
            throw new Error("AI response was empty.");
        }

    } catch (error) {
        console.error("Error in getSpokenExplanation (Step 1 - AI Text):", error);
        // Fallback: If AI fails, just speak the selected text
        explanationText = text;
    }

    // 3. === Step 2: Convert text to speech (USING OPENAI) ===
    try {
        // We re-use the 'openai' client initialized in Step 1
        const response = await openai.audio.speech.create({
            model: "tts-1",
            voice: "nova", // You can also try "alloy", "echo", "fable", "onyx", "shimmer"
            input: explanationText,
            response_format: "mp3",
        });

        // 4. === Step 3: Return the audio data as base64 ===
        const audioBuffer = Buffer.from(await response.arrayBuffer());
        
        return {
            audioBase64: audioBuffer.toString('base64')
        };

    } catch (error) {
        console.error("Error in getSpokenExplanation (Step 2 - OpenAI TTS):", error);
        throw new functions.https.HttpsError("internal", "Failed to generate spoken audio.");
    }
});

// ─────────────────────────────────────────────────────────────
// ✅ TOEFL Speaking Practice Functions — (Already 2nd Gen, no change)
// ─────────────────────────────────────────────────────────────
exports.getTOEFLSpeakingPrompt = onCall({ secrets: [openAIKey] }, async (request) => {
  if (!request.auth) {
    throw new functions.https.HttpsError("unauthenticated", "You must be signed in.");
  }
  const openai = new OpenAI({ apiKey: openAIKey.value() });
  try {
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content: "You create TOEFL Speaking Task 1 prompts. The prompt must be a short 'agree or disagree' or 'preference' question. Provide only the question text."
        },
        { role: "user", content: "Generate a new TOEFL speaking prompt." }
      ],
      temperature: 1.1,
      max_tokens: 60,
    });
    const prompt = response.choices[0]?.message?.content.trim() || "Do you agree that mobile phones improve our quality of life?";
    return { prompt };
  } catch (error) {
    console.error("Error in getTOEFLSpeakingPrompt:", error);
    throw new functions.https.HttpsError("internal", "Failed to generate a prompt.");
  }
});

exports.analyzeTOEFLText = onCall({ secrets: [openAIKey] }, async (request) => {
  if (!request.auth) {
    throw new functions.https.HttpsError("unauthenticated", "You must be signed in to analyze text.");
  }
  const { text, prompt } = request.data;
  if (!text || !prompt) {
    throw new functions.https.HttpsError("invalid-argument", 'Missing required fields: "text" and "prompt".');
  }
  if (text.trim().length < 5) {
      return {
          feedback: "Your response seemed a bit short. Could you please try speaking a little longer and more clearly?",
          transcript: text || "(No speech was detected.)"
      };
  }
  const openai = new OpenAI({ apiKey: openAIKey.value() });
  try {
    const analysisResponse = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content: "You are a friendly and encouraging TOEFL speaking coach. Analyze the student's response based on the provided transcript. Provide constructive feedback in 2-3 concise sentences, focusing on fluency, coherence, and grammar. Start with a positive comment. Do not give a score. Respond in plain text."
        },
        {
          role: "user",
          content: `Original Prompt: "${prompt}"\n\nStudent's Transcribed Response: "${text}"`
        }
      ],
      temperature: 0.3,
      max_tokens: 250,
    });
    const feedback = analysisResponse.choices[0]?.message?.content || "Great effort! Keep practicing to improve your confidence and fluency.";
    return { feedback, transcript: text };
  } catch (error) {
    console.error("Detailed error in analyzeTOEFLText:", JSON.stringify(error, null, 2));
    throw new functions.https.HttpsError("internal", "We couldn't analyze your response right now. Please try again.");
  }
});


// ─────────────────────────────────────────────────────────────
// ✅ Kaffetish KPI Aggregation Function — (Already 2nd Gen, no change)
// ─────────────────────────────────────────────────────────────
exports.aggregateKpiStats = onDocumentWritten("reading_sessions/{sessionId}", async (event) => {
    const sessionData = event.data.after.data();
    if (!sessionData.endTime) {
        console.log(`Session ${event.params.sessionId} started or is ongoing. No action needed.`);
        return null;
    }
    const summaryRef = db.doc("public_summary/kpi");
    try {
        const summaryDoc = await summaryRef.get();
        if (summaryDoc.exists) {
            const lastRun = summaryDoc.data().lastUpdated.toDate();
            const now = new Date();
            const minutesSinceLastRun = (now.getTime() - lastRun.getTime()) / (1000 * 60);
            if (minutesSinceLastRun < 60) {
                console.log(`Aggregation ran ${minutesSinceLastRun.toFixed(1)} minutes ago. It's too soon to run again.`);
                return null;
            }
        }
    } catch (e) {
        console.log("Could not find previous summary doc, proceeding with aggregation.");
    }
    console.log("Cooldown period has passed. Running the full KPI aggregation job...");
    const now = new Date();
    const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000);
    const thirtyDaysAgo = new Date(now.getTime() - 30 * 24 * 60 * 60 * 1000);
    const usersSnapshot = await db.collection("users").get();
    const totalUsers = usersSnapshot.size;
    let dau = 0, mau = 0;
    usersSnapshot.forEach(doc => {
      if (doc.data().lastSeen && doc.data().lastSeen.toDate) {
        const lastSeen = doc.data().lastSeen.toDate();
        if (lastSeen >= oneDayAgo) dau++;
        if (lastSeen >= thirtyDaysAgo) mau++;
      }
    });
    const sessionsSnapshot = await db.collection("reading_sessions").get();
    let totalMillisRead = 0;
    const bookStats = new Map();
    sessionsSnapshot.forEach(doc => {
      const data = doc.data();
      if (data.startTime && data.endTime) {
        const millis = data.endTime.toMillis() - data.startTime.toMillis();
        totalMillisRead += millis;
        const current = bookStats.get(data.bookTitle) || { totalMillis: 0, opens: 0 };
        current.totalMillis += millis;
        current.opens += 1;
        bookStats.set(data.bookTitle, current);
      }
    });
    const totalReadingHours = totalMillisRead / (1000 * 60 * 60);
    const explanationsSnapshot = await db.collection("explanation_requests").get();
    const totalExplanations = explanationsSnapshot.size;
    await summaryRef.set({
      totalReadingHours: parseFloat(totalReadingHours.toFixed(2)),
      totalExplanations, dau, mau, totalUsers,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp(),
    }, { merge: true });
    const batch = db.batch();
    for (const [title, stats] of bookStats.entries()) {
      const slug = title.replace(/\s+/g, '-').toLowerCase();
      const bookRef = db.doc(`public_book_stats/${slug}`);
      batch.set(bookRef, {
        title: title,
        totalHours: parseFloat((stats.totalMillis / (1000 * 60 * 60)).toFixed(2)),
        totalOpens: stats.opens,
      }, { merge: true });
    }
    await batch.commit();
    console.log("On-demand KPI aggregation job finished successfully.");
    return null;
}); 
